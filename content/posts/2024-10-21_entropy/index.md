---
date: '2024-10-21T21:19:34-04:00'
draft: false
title: 'Entropy and the Quantification of Information'
mathjax: true
tags: ['Information Theory', 'Entropy']
---

![A room with high entropy](./high-entropy.jpeg)

What is entropy? In one line, **entropy is a measure of the amount of information**, or to put it another way, **a measure of the level of suprise**. But what does that means? In specific, we may ask

1. What is information, or what entity has information?

2. Can one entity has more information than another? What does that even mean?

## Information Content

## Entropy