---
layout: math_post
title: Entropy and the Quantification of Information
date: 2024-10-19
categories: [blog]
tags: [Information Theory, Entropy]
header_image: ./high-entropy.jpeg
---

What is entropy? In one line, **entropy is a measure of the amount of information**, or to put it another way, **a measure of the level of suprise**. But what does that means? In specific, we may ask

1. What is information, or what entity has information?

2. Rigorously, what does it mean to say there is "a lot of information" or "barely no information"

For the first question, loosely speaking, these two concepts from probability theory can have information:

- an event

- a random variable

Say, we have a random variable `X` that describes the  

## Information Content

## Entropy